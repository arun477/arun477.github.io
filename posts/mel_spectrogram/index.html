<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mel Spectrogram | Arun</title>
<meta name=keywords content="audio,signal-processing"><meta name=description content="Introduction
Mel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models&rsquo; input raw audio is converted into mel spectrogram before passing to the model.
In general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form."><meta name=author content="Arun"><link rel=canonical href=https://arun477.github.io/posts/mel_spectrogram/><link crossorigin=anonymous href=/assets/css/stylesheet.c5d717fd95f16135f2524db5303aac3beafb1f69ea63a7be8dc2e1f263f8f2e1.css integrity="sha256-xdcX/ZXxYTXyUk21MDqsO+r7H2nqY6e+jcLh8mP48uE=" rel="preload stylesheet" as=style><link rel=icon href=https://arun477.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://arun477.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://arun477.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://arun477.github.io/apple-touch-icon.png><link rel=mask-icon href=https://arun477.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://arun477.github.io/posts/mel_spectrogram/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/contrib/auto-render.min.js onload='document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})'></script><meta property="og:url" content="https://arun477.github.io/posts/mel_spectrogram/"><meta property="og:site_name" content="Arun"><meta property="og:title" content="Mel Spectrogram"><meta property="og:description" content="Introduction Mel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models’ input raw audio is converted into mel spectrogram before passing to the model.
In general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-23T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-23T00:00:00+00:00"><meta property="article:tag" content="Audio"><meta property="article:tag" content="Signal-Processing"><meta property="og:image" content="https://arun477.github.io/images/mel_spectrogram.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://arun477.github.io/images/mel_spectrogram.jpeg"><meta name=twitter:title content="Mel Spectrogram"><meta name=twitter:description content="Introduction
Mel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models&rsquo; input raw audio is converted into mel spectrogram before passing to the model.
In general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://arun477.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Mel Spectrogram","item":"https://arun477.github.io/posts/mel_spectrogram/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mel Spectrogram","name":"Mel Spectrogram","description":"Introduction Mel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models\u0026rsquo; input raw audio is converted into mel spectrogram before passing to the model.\nIn general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form.\n","keywords":["audio","signal-processing"],"articleBody":"Introduction Mel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models’ input raw audio is converted into mel spectrogram before passing to the model.\nIn general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form.\nTo fully understand how this conversion process occurs, what exactly is going on under the hood, and why we need this conversion, we need to understand a few other related concepts and techniques surrounding audio in general.\nHow Audio is Represented Digitally In a physical sense, audio signals are continuous signals, but we usually represent this audio in discrete form when we represent it digitally. It is basically a list of numbers, usually 16-bit integers (if it’s a .wav file).\nEach number in this list basically records the amplitude (strength of the physical sound waves) value captured by the microphone at various time points.\nSampling Rate Now here, one parameter comes into the picture, which is how many samples the microphone should record in a given time period. For example, if our microphone records a lot of samples per second, then our audio quality would be much better, but there is a limit. This is usually 22k samples per second or 44k samples per second. These rates are enough to capture most of the audio frequency and quality. This rate is called the sampling rate.\nNyquist Theorem (Mathematical Guarantee) Now another question comes up: why do these choices usually work, like 22k samples per second or 44k samples per second? There is one famous mathematical theorem called the Nyquist theorem, which provides strong theoretical guarantees. It says that if you want to capture higher frequencies, then your sampling rate should be 2 times the highest frequency you are trying to capture.\n$$f_{max} = \\frac{f_s}{2}$$\nWhere: $$\\begin{aligned} f_{max} \u0026= \\text{highest frequency (Nyquist frequency)} \\ f_s \u0026= \\text{sampling rate (Hz)} \\end{aligned}$$\nUsually, the human hearing range is 20 Hz to 20 kHz, so a sampling rate of 40 kHz will capture up to 20 kHz. This is enough for most cases. Sometimes music requires a slightly higher frequency range, so accordingly, a higher sampling rate is required.\nNotational Meaning Difference There is a slight notational difference here. Hz means different things when we talk about frequency and sampling rate. When we say 40 kHz, we are talking about 40k samples per second, and when we say 40 kHz in frequency, we are talking about cycles per second.\nTime Domain Signal Now this array of signal captures all required information of the audio signal. We can transmit it and play it using a speaker. This is called time domain signal because it is basically a signal across the time.\nFrequency Domain Signal One of the interesting and key ideas is that we can pick a particular instant in time and take the samples from that instant to check what are all the frequencies available.\nThis is done through another cool mathematical technique called Fourier transform which takes the samples of the audio at a particular instant in time and gives all the available frequency information at that instant. To be exact, it will give all the frequencies with their magnitude and phase information (usually complex numbers).\nOf course, the maximum frequency it can extract is bound by Nyquist theorem, basically sampling rate / 2 is the maximum frequency we can extract.\nSpectrogram Now if we take this frequency information across different time instants, we will get frequency information at each instant of time across the time axis. This is called spectrogram.\nWe usually do two additional transformations on this spectrogram.\nTaking only amplitude value. Fourier transform returns frequency value as a complex number which contains both amplitude and phase information of each frequency. Usually, we discard phase information and only keep amplitude information. This is done by taking the Euclidean norm of the real and imaginary parts.\nFinal transform is converting this amplitude value to decibel value, basically taking each amplitude value and taking the log of it. The reason seems to be that this closely matches how humans perceive the audio amplitude. To be exact, the following is the transformation formula.\n$$\\text{dB} = 20 \\log_{10}\\left(\\frac{|A|}{A_{\\text{ref}}}\\right)$$\nWhere: $$\\begin{aligned} \\text{dB} \u0026= \\text{amplitude in decibels} \\ |A| \u0026= \\text{absolute value of amplitude} \\ A_{\\text{ref}} \u0026= \\text{reference amplitude value} \\end{aligned}$$\n$$\\text{dB} = 10 \\log_{10}\\left(\\frac{|X(f)|^2}{X_{\\text{ref}}^2}\\right)$$\nOr equivalently: $$\\text{dB} = 20 \\log_{10}\\left(\\frac{|X(f)|}{X_{\\text{ref}}}\\right)$$\nWhere: $$\\begin{aligned} X(f) \u0026= \\text{Fourier transform at frequency } f \\ |X(f)| \u0026= \\text{magnitude of the transform} \\ X_{\\text{ref}} \u0026= \\text{reference value} \\end{aligned}$$\nMel Spectrogram The spectrogram we have seen so far gives information about each instant, what are all the frequencies, and how it changes across the time.\nThere is another interesting fact: humans can’t very well differentiate between higher frequencies the way they do in lower frequency ranges. Basically, human frequency perception kind of follows a logarithmic function which implies if we take one instant of spectrogram frequency information, they usually have linear spacing, meaning 20 Hz, 40 Hz, 60 Hz, etc. (in this example 20 Hz apart). But as we move to higher frequency ranges, the human ear can’t distinguish this. Meaning the human ear can distinguish the difference between 20 Hz and 40 Hz but it can’t between 12 kHz and 12.20 kHz, even though the same space between both frequencies. So there is a lot of unnecessary frequency information (in terms of human perception point of view).\nWhich implies we can group a lot of the frequency based on this idea. This is done by first picking a set of frequency bands, usually 80 bands, which basically focus on different frequency ranges from 0 to max frequency. Each frequency band will have a set of weights which tries to capture human perception. Lower frequency weights will be mostly zero weights for far away frequencies and less weights for nearby frequencies. In higher frequency bands, weights will be dispersed because there we can combine a lot of frequencies.\nThis mel spectrogram process might not be obvious but when we go through the full code it will get clarified. But for now, it’s kind of a dimensionality reduction technique on spectrogram which takes into account how humans perceive sounds.\nCode Let’s do the full conversion process in code to see what exactly is going on. For most of the audio processing, let’s use a library called librosa which contains a lot of useful utility functions to work with the audio data and along with some sample audio data to work with.\nFirst, let’s import required libraries and let’s use libri1 sample audio from librosa library which is basically human voice.\nimport librosa import numpy as np import matplotlib.pyplot as plt import IPython as ipy audio, sr = librosa.load(librosa.ex('libri1')) In the above code, sr is a sampling rate and audio is our actual audio data which is a 1-dimensional array of numbers.\nFollowing is the original audio.\nYour browser does not support the audio element. Setup FFT Parameters n_fft = 1024 hop_length = 512 Frequency Resolution We know maximum frequency possible by Nyquist theorem is sampling_rate/2, so FFT can produce up to sampling_rate/2 frequency. Another fact is FFT algorithm produces symmetric values so only n_fft/2 bins are unique frequency bins. FFT algorithm works by evenly spaced frequency bins, so the spacing becomes (sampling_rate/2) / (n_fft/2) = sampling_rate/n_fft.\nfrq_resolution = sr/n_fft print(f'frq resolution: {frq_resolution:.0f} Hz') Time Resolution 1/sampling_rate will tell us spacing between individual sample points. n_fft * (1/sampling_rate) will tell us the time bracket the FFT instance occupies.\ntime_resolution = n_fft/sr print(f'time resolution: {time_resolution*1000:.0f} ms') There is a trade-off here. If we increase the time resolution, then we have to sacrifice the frequency resolution. So balance is required.\nFrequency Bins Calculation freq_bins = (n_fft//2) + 1 print(f\"frequency bins: {freq_bins}\") Time Frame Calculation How many time frames for the given audio signals after STFT. This is like convolution kernel operation.\ntotal_time_frame_points = int((1+(len(audio)-n_fft)/hop_length)) print(f'estimated total time frame points: {total_time_frame_points:.0f}') Short-Time Fourier Transform (STFT) D = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length) print(f\"stft output: {D.shape} = (frq_bins, time_frame_points)\") Amplitude Extraction STFT contains both amplitude and phase information. Usually we take only amplitude information and discard the phase information. There is some subtlety here - np.abs for real part works just by taking normal abs, but if it’s a complex number, in this case it will calculate abs by taking Euclidean norm of the real and complex parts. Equivalent in PyTorch would be the following: x = torch.tensor([D[1][0].real, D[1][0].imag]) torch.sqrt(x.pow(2).sum(-1))\nAmplitude is the strength of each frequency bin which is the one visualized in spectrograms.\nD_amp = np.abs(D) Logarithmic Transformation Taking log of these amplitude values gives values that are more suitable to how humans perceive audio and relative difference between different frequency strengths. It won’t be exact log but close enough (actual formula will look like: 20 * log10(amplitude / reference)). The ref=np.max is basically used to normalize the audio to 1 range and take the log which shifts the values to negative side and max value will be 0, and this is done for conventional reasons.\nS_db = librosa.amplitude_to_db(D_amp, ref=np.max) Display Spectrogram librosa.display.specshow(S_db, x_axis='time', y_axis='hz', hop_length=hop_length) plt.colorbar(format='%+2.0f dB') plt.show() Audio Reconstruction with Griffin-Lim Algorithm If we want to convert this spectrogram back to the original audio signal, we face one caveat: during amplitude calculation we discarded the phase information. Phase information is crucial for the reconstruction.\nOne classical way to estimate that missing phase information is using an iterative algorithm which works like this:\nAssign random phase for the magnitude Calculate inverse short time Fourier transform (ISTFT) which will give audio signal Now calculate spectrogram from this new audio signal Compare the magnitude with original magnitude (kind of loss) Now estimate the new phase which reduces this loss Repeat this until loss is low This algorithm is called the Griffin-Lim algorithm.\nD_amp_reconstructed = librosa.db_to_amplitude(S_db) reconstructed_audio = librosa.griffinlim(D_amp_reconstructed, n_iter=32, hop_length=hop_length, win_length=n_fft) print(f\"reconstructed audio shape: {reconstructed_audio.shape}\") ipy.display.Audio(reconstructed_audio, rate=sr) Reconstructed Audio from Spectrogram Download the reconstructed audio file You can hear that the reconstructed audio is not exactly the same as the original. This is because the phase information was estimated rather than preserved from the original signal. The Griffin-Lim algorithm provides a reasonable approximation, but there will always be some differences compared to the original audio.\nMel Spectrogram Generation For mel spectrogram we have to take STFT output and convert to amplitude and take the power (which basically squares all the values). The reason why we take square is we want energy which is square of the amplitude. The reason why we need energy instead of amplitude is because human perception is better with power than amplitude.\nfrom librosa.filters import mel power_sepc = np.abs(D)**2 print(f\"power spec shape: {power_sepc.shape}\") Mel filter bank is basically dimensionality reduction of the power spectrogram. These filters are created as follows:\nWe take 80 evenly spaced points from fmin=(0) to fmax= sr/2 Note these evenly spaced points are in log scale (exact formula: 2595 * log10(1 + hz/700)) Now take these log scale 80 points to linear scale (exact conversion formula: 700 * (10^(mel/2595) - 1)) This will be the center points for the filters in our frequency bins These filter values are peak at the center point and low at the edges like triangle Then we apply this filter across frequency bins dimension to get the final bin value Basically lower frequency weight reduction will be steep because human perception in lower frequency range is good but higher frequency weight reduction will decrease slowly because in higher frequency range human perception of distinction becomes hard.\nn_mels = 80 mel_filterbank = mel(sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=0, fmax=sr/2) print(f\"mel filter bank shape: {mel_filterbank.shape}\") mel_spectogram = np.dot(mel_filterbank, power_sepc) print(f\"mel spec shape: {mel_spectogram.shape}\") Let’s convert back to dB scale. This is to match human loudness perception.\n# this to match human loudness perception matching mel_S_db = librosa.power_to_db(mel_spectogram, ref=np.max) librosa.display.specshow(mel_S_db, x_axis='time', y_axis='mel', sr=sr, hop_length=hop_length) plt.colorbar(format='%+2.0f dB') plt.tight_layout() plt.show() Figure: Mel Spectrogram with 80 mel bands. Notice how the frequency axis is now non-linear with more resolution in lower frequencies.\nYou can see how the Mel spectrogram differs from the regular STFT spectrogram we generated earlier. The Mel spectrogram has more resolution in the lower frequency ranges, which better matches human perception of sound. This is why Mel spectrograms are commonly used in speech recognition and music analysis applications.\nMel Spectrogram Inversion Now let’s try to convert our mel spectrogram back to audio:\n# power to db mel_power = librosa.db_to_power(mel_S_db) # mel filterbank which is used for the conversion. mel_fb = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels) # take inverse of the mel. # this will be less accurate as after mel conversion we lost some of the original information. mel_inverse = np.linalg.pinv(mel_fb) linear_spec_approx = np.dot(mel_inverse, mel_power) print(f'linear spec approximate shape: {linear_spec_approx.shape}') # power can't have negative values. so clip it. linear_spec_approx = np.maximum(0, linear_spec_approx) # this reconstruction may not be as accurate like the original direct spec to audio # conversion because we lose information when we do this mel conversion. mel_reconstructed_audio = librosa.griffinlim(linear_spec_approx, n_iter=32, hop_length=hop_length, win_length=n_fft) ipy.display.Audio(mel_reconstructed_audio, rate=sr) Audio Reconstructed from Mel Spectrogram Download the Mel reconstructed audio file Notice how the audio quality from the Mel spectrogram reconstruction is generally lower than the reconstruction from the original STFT. This is because the Mel transformation is a dimensionality reduction process - we compress the frequency information from the full STFT bins (which was 513 frequency bins) down to just 80 Mel bands.\nThe process involves:\nConverting our dB scale Mel spectrogram back to power Getting the original Mel filterbank matrix Computing the pseudo-inverse of the Mel filterbank Using the pseudo-inverse to estimate the original linear spectrogram Ensuring all values are non-negative (power can’t be negative) Using Griffin-Lim algorithm to estimate the phase and reconstruct the audio The Mel transformation deliberately discards information that isn’t perceptually significant to humans, making it excellent for tasks like speech recognition but less suitable for high-fidelity audio reproduction.\nReferences Wikipedia. “Nyquist–Shannon sampling theorem.” [Online]. Available: https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\nHugging Face. “Audio Course - Introduction.” [Online]. Available: https://huggingface.co/learn/audio-course/chapter0/introduction\n3Blue1Brown. “But what is the Fourier Transform? A visual introduction.” YouTube. [Online]. Available: https://youtu.be/spUNpyF58BY\nLibrosa Documentation. [Online]. Available: https://librosa.org/doc/latest/index.html\n","wordCount":"2394","inLanguage":"en","image":"https://arun477.github.io/images/mel_spectrogram.jpeg","datePublished":"2025-03-23T00:00:00Z","dateModified":"2025-03-23T00:00:00Z","author":{"@type":"Person","name":"Arun"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://arun477.github.io/posts/mel_spectrogram/"},"publisher":{"@type":"Organization","name":"Arun","logo":{"@type":"ImageObject","url":"https://arun477.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://arun477.github.io/ accesskey=h title="Arun (Alt + H)">Arun</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://arun477.github.io/ title=Home><span>Home</span></a></li><li><a href=https://arun477.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://arun477.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://arun477.github.io/utilities/ title=Utilities><span>Utilities</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://arun477.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://arun477.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Mel Spectrogram</h1><div class=post-meta><span title='2025-03-23 00:00:00 +0000 UTC'>March 23, 2025</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2394 words&nbsp;·&nbsp;Arun</div></header><figure class=entry-cover><img loading=eager src=https://arun477.github.io/images/mel_spectrogram.jpeg alt="Mel Spectrogram Banner"><figcaption>Mel Spectrogram</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#how-audio-is-represented-digitally>How Audio is Represented Digitally</a></li><li><a href=#sampling-rate>Sampling Rate</a></li><li><a href=#nyquist-theorem-mathematical-guarantee>Nyquist Theorem (Mathematical Guarantee)</a></li><li><a href=#notational-meaning-difference>Notational Meaning Difference</a></li><li><a href=#time-domain-signal>Time Domain Signal</a></li><li><a href=#frequency-domain-signal>Frequency Domain Signal</a></li><li><a href=#spectrogram>Spectrogram</a></li><li><a href=#mel-spectrogram>Mel Spectrogram</a></li><li><a href=#code>Code</a><ul><li><a href=#setup-fft-parameters>Setup FFT Parameters</a></li><li><a href=#frequency-resolution>Frequency Resolution</a></li><li><a href=#time-resolution>Time Resolution</a></li><li><a href=#frequency-bins-calculation>Frequency Bins Calculation</a></li><li><a href=#time-frame-calculation>Time Frame Calculation</a></li><li><a href=#short-time-fourier-transform-stft>Short-Time Fourier Transform (STFT)</a></li><li><a href=#amplitude-extraction>Amplitude Extraction</a></li><li><a href=#logarithmic-transformation>Logarithmic Transformation</a></li><li><a href=#display-spectrogram>Display Spectrogram</a></li><li><a href=#audio-reconstruction-with-griffin-lim-algorithm>Audio Reconstruction with Griffin-Lim Algorithm</a></li><li><a href=#mel-spectrogram-generation>Mel Spectrogram Generation</a></li><li><a href=#mel-spectrogram-inversion>Mel Spectrogram Inversion</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Mel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models&rsquo; input raw audio is converted into mel spectrogram before passing to the model.</p><p>In general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form.</p><p>To fully understand how this conversion process occurs, what exactly is going on under the hood, and why we need this conversion, we need to understand a few other related concepts and techniques surrounding audio in general.</p><h2 id=how-audio-is-represented-digitally>How Audio is Represented Digitally<a hidden class=anchor aria-hidden=true href=#how-audio-is-represented-digitally>#</a></h2><p>In a physical sense, audio signals are continuous signals, but we usually represent this audio in discrete form when we represent it digitally. It is basically a list of numbers, usually 16-bit integers (if it&rsquo;s a .wav file).</p><p>Each number in this list basically records the amplitude (strength of the physical sound waves) value captured by the microphone at various time points.</p><h2 id=sampling-rate>Sampling Rate<a hidden class=anchor aria-hidden=true href=#sampling-rate>#</a></h2><p>Now here, one parameter comes into the picture, which is how many samples the microphone should record in a given time period. For example, if our microphone records a lot of samples per second, then our audio quality would be much better, but there is a limit. This is usually 22k samples per second or 44k samples per second. These rates are enough to capture most of the audio frequency and quality. This rate is called the <strong>sampling rate</strong>.</p><h2 id=nyquist-theorem-mathematical-guarantee>Nyquist Theorem (Mathematical Guarantee)<a hidden class=anchor aria-hidden=true href=#nyquist-theorem-mathematical-guarantee>#</a></h2><p>Now another question comes up: why do these choices usually work, like 22k samples per second or 44k samples per second? There is one famous mathematical theorem called the <strong>Nyquist theorem</strong>, which provides strong theoretical guarantees. It says that if you want to capture higher frequencies, then your sampling rate should be 2 times the highest frequency you are trying to capture.</p><p>$$f_{max} = \frac{f_s}{2}$$</p><p>Where:
$$\begin{aligned}
f_{max} &= \text{highest frequency (Nyquist frequency)} \
f_s &= \text{sampling rate (Hz)}
\end{aligned}$$</p><p>Usually, the human hearing range is 20 Hz to 20 kHz, so a sampling rate of 40 kHz will capture up to 20 kHz. This is enough for most cases. Sometimes music requires a slightly higher frequency range, so accordingly, a higher sampling rate is required.</p><h2 id=notational-meaning-difference>Notational Meaning Difference<a hidden class=anchor aria-hidden=true href=#notational-meaning-difference>#</a></h2><p>There is a slight notational difference here. Hz means different things when we talk about frequency and sampling rate. When we say 40 kHz, we are talking about 40k samples per second, and when we say 40 kHz in frequency, we are talking about cycles per second.</p><h2 id=time-domain-signal>Time Domain Signal<a hidden class=anchor aria-hidden=true href=#time-domain-signal>#</a></h2><p>Now this array of signal captures all required information of the audio signal. We can transmit it and play it using a speaker. This is called <strong>time domain signal</strong> because it is basically a signal across the time.</p><h2 id=frequency-domain-signal>Frequency Domain Signal<a hidden class=anchor aria-hidden=true href=#frequency-domain-signal>#</a></h2><p>One of the interesting and key ideas is that we can pick a particular instant in time and take the samples from that instant to check what are all the frequencies available.</p><p>This is done through another cool mathematical technique called <strong>Fourier transform</strong> which takes the samples of the audio at a particular instant in time and gives all the available frequency information at that instant. To be exact, it will give all the frequencies with their magnitude and phase information (usually complex numbers).</p><p>Of course, the maximum frequency it can extract is bound by Nyquist theorem, basically sampling rate / 2 is the maximum frequency we can extract.</p><h2 id=spectrogram>Spectrogram<a hidden class=anchor aria-hidden=true href=#spectrogram>#</a></h2><p>Now if we take this frequency information across different time instants, we will get frequency information at each instant of time across the time axis. This is called <strong>spectrogram</strong>.</p><p>We usually do two additional transformations on this spectrogram.</p><p>Taking only amplitude value. Fourier transform returns frequency value as a complex number which contains both amplitude and phase information of each frequency. Usually, we discard phase information and only keep amplitude information. This is done by taking the Euclidean norm of the real and imaginary parts.</p><p>Final transform is converting this amplitude value to decibel value, basically taking each amplitude value and taking the log of it. The reason seems to be that this closely matches how humans perceive the audio amplitude. To be exact, the following is the transformation formula.</p><p>$$\text{dB} = 20 \log_{10}\left(\frac{|A|}{A_{\text{ref}}}\right)$$</p><p>Where:
$$\begin{aligned}
\text{dB} &= \text{amplitude in decibels} \
|A| &= \text{absolute value of amplitude} \
A_{\text{ref}} &= \text{reference amplitude value}
\end{aligned}$$</p><p>$$\text{dB} = 10 \log_{10}\left(\frac{|X(f)|^2}{X_{\text{ref}}^2}\right)$$</p><p>Or equivalently:
$$\text{dB} = 20 \log_{10}\left(\frac{|X(f)|}{X_{\text{ref}}}\right)$$</p><p>Where:
$$\begin{aligned}
X(f) &= \text{Fourier transform at frequency } f \
|X(f)| &= \text{magnitude of the transform} \
X_{\text{ref}} &= \text{reference value}
\end{aligned}$$</p><h2 id=mel-spectrogram>Mel Spectrogram<a hidden class=anchor aria-hidden=true href=#mel-spectrogram>#</a></h2><p>The spectrogram we have seen so far gives information about each instant, what are all the frequencies, and how it changes across the time.</p><p>There is another interesting fact: humans can&rsquo;t very well differentiate between higher frequencies the way they do in lower frequency ranges. Basically, human frequency perception kind of follows a logarithmic function which implies if we take one instant of spectrogram frequency information, they usually have linear spacing, meaning 20 Hz, 40 Hz, 60 Hz, etc. (in this example 20 Hz apart). But as we move to higher frequency ranges, the human ear can&rsquo;t distinguish this. Meaning the human ear can distinguish the difference between 20 Hz and 40 Hz but it can&rsquo;t between 12 kHz and 12.20 kHz, even though the same space between both frequencies. So there is a lot of unnecessary frequency information (in terms of human perception point of view).</p><p>Which implies we can group a lot of the frequency based on this idea. This is done by first picking a set of frequency bands, usually 80 bands, which basically focus on different frequency ranges from 0 to max frequency. Each frequency band will have a set of weights which tries to capture human perception. Lower frequency weights will be mostly zero weights for far away frequencies and less weights for nearby frequencies. In higher frequency bands, weights will be dispersed because there we can combine a lot of frequencies.</p><p>This mel spectrogram process might not be obvious but when we go through the full code it will get clarified. But for now, it&rsquo;s kind of a dimensionality reduction technique on spectrogram which takes into account how humans perceive sounds.</p><h2 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h2><p>Let&rsquo;s do the full conversion process in code to see what exactly is going on. For most of the audio processing, let&rsquo;s use a library called <strong>librosa</strong> which contains a lot of useful utility functions to work with the audio data and along with some sample audio data to work with.</p><p>First, let&rsquo;s import required libraries and let&rsquo;s use <strong>libri1</strong> sample audio from librosa library which is basically human voice.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e67e80>import</span> librosa
</span></span><span style=display:flex><span><span style=color:#e67e80>import</span> numpy <span style=color:#e67e80>as</span> np
</span></span><span style=display:flex><span><span style=color:#e67e80>import</span> matplotlib.pyplot <span style=color:#e67e80>as</span> plt
</span></span><span style=display:flex><span><span style=color:#e67e80>import</span> IPython <span style=color:#e67e80>as</span> ipy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>audio, sr <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>load(librosa<span style=color:#7a8478>.</span>ex(<span style=color:#b2c98f>&#39;libri1&#39;</span>))
</span></span></code></pre></div><p>In the above code, <strong>sr</strong> is a sampling rate and <strong>audio</strong> is our actual audio data which is a 1-dimensional array of numbers.</p><p>Following is the original audio.</p><div class=custom-html-container><audio controls><source src=/audio/libri1_voice.wav type=audio/mpeg>Your browser does not support the audio element.</audio></div><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("");if(e){const t=e.getElementsByTagName("script");for(let e=0;e<t.length;e++){const n=document.createElement("script");t[e].src?n.src=t[e].src:n.textContent=t[e].textContent,document.body.appendChild(n)}}})</script><h3 id=setup-fft-parameters>Setup FFT Parameters<a hidden class=anchor aria-hidden=true href=#setup-fft-parameters>#</a></h3><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n_fft <span style=color:#7a8478>=</span> <span style=color:#d699b6>1024</span>
</span></span><span style=display:flex><span>hop_length <span style=color:#7a8478>=</span> <span style=color:#d699b6>512</span>
</span></span></code></pre></div><h3 id=frequency-resolution>Frequency Resolution<a hidden class=anchor aria-hidden=true href=#frequency-resolution>#</a></h3><p>We know maximum frequency possible by Nyquist theorem is sampling_rate/2, so FFT can produce up to sampling_rate/2 frequency. Another fact is FFT algorithm produces symmetric values so only n_fft/2 bins are unique frequency bins. FFT algorithm works by evenly spaced frequency bins, so the spacing becomes (sampling_rate/2) / (n_fft/2) = sampling_rate/n_fft.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>frq_resolution <span style=color:#7a8478>=</span> sr<span style=color:#7a8478>/</span>n_fft
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#39;frq resolution: </span><span style=color:#b2c98f>{</span>frq_resolution<span style=color:#b2c98f>:</span><span style=color:#b2c98f>.0f</span><span style=color:#b2c98f>}</span><span style=color:#b2c98f> Hz&#39;</span>)
</span></span></code></pre></div><h3 id=time-resolution>Time Resolution<a hidden class=anchor aria-hidden=true href=#time-resolution>#</a></h3><p>1/sampling_rate will tell us spacing between individual sample points. n_fft * (1/sampling_rate) will tell us the time bracket the FFT instance occupies.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>time_resolution <span style=color:#7a8478>=</span> n_fft<span style=color:#7a8478>/</span>sr
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#39;time resolution: </span><span style=color:#b2c98f>{</span>time_resolution<span style=color:#7a8478>*</span><span style=color:#d699b6>1000</span><span style=color:#b2c98f>:</span><span style=color:#b2c98f>.0f</span><span style=color:#b2c98f>}</span><span style=color:#b2c98f> ms&#39;</span>)
</span></span></code></pre></div><p>There is a trade-off here. If we increase the time resolution, then we have to sacrifice the frequency resolution. So balance is required.</p><h3 id=frequency-bins-calculation>Frequency Bins Calculation<a hidden class=anchor aria-hidden=true href=#frequency-bins-calculation>#</a></h3><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>freq_bins <span style=color:#7a8478>=</span> (n_fft<span style=color:#7a8478>//</span><span style=color:#d699b6>2</span>) <span style=color:#7a8478>+</span> <span style=color:#d699b6>1</span>
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#34;frequency bins: </span><span style=color:#b2c98f>{</span>freq_bins<span style=color:#b2c98f>}</span><span style=color:#b2c98f>&#34;</span>)
</span></span></code></pre></div><h3 id=time-frame-calculation>Time Frame Calculation<a hidden class=anchor aria-hidden=true href=#time-frame-calculation>#</a></h3><p>How many time frames for the given audio signals after STFT. This is like convolution kernel operation.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>total_time_frame_points <span style=color:#7a8478>=</span> <span style=color:#d699b6>int</span>((<span style=color:#d699b6>1</span><span style=color:#7a8478>+</span>(<span style=color:#d699b6>len</span>(audio)<span style=color:#7a8478>-</span>n_fft)<span style=color:#7a8478>/</span>hop_length))
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#39;estimated total time frame points: </span><span style=color:#b2c98f>{</span>total_time_frame_points<span style=color:#b2c98f>:</span><span style=color:#b2c98f>.0f</span><span style=color:#b2c98f>}</span><span style=color:#b2c98f>&#39;</span>)
</span></span></code></pre></div><h3 id=short-time-fourier-transform-stft>Short-Time Fourier Transform (STFT)<a hidden class=anchor aria-hidden=true href=#short-time-fourier-transform-stft>#</a></h3><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>D <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>stft(audio, n_fft<span style=color:#7a8478>=</span>n_fft, hop_length<span style=color:#7a8478>=</span>hop_length)
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#34;stft output: </span><span style=color:#b2c98f>{</span>D<span style=color:#7a8478>.</span>shape<span style=color:#b2c98f>}</span><span style=color:#b2c98f> = (frq_bins, time_frame_points)&#34;</span>)
</span></span></code></pre></div><h3 id=amplitude-extraction>Amplitude Extraction<a hidden class=anchor aria-hidden=true href=#amplitude-extraction>#</a></h3><p>STFT contains both amplitude and phase information. Usually we take only amplitude information and discard the phase information. There is some subtlety here - np.abs for real part works just by taking normal abs, but if it&rsquo;s a complex number, in this case it will calculate abs by taking Euclidean norm of the real and complex parts. Equivalent in PyTorch would be the following:
x = torch.tensor([D[1][0].real, D[1][0].imag])
torch.sqrt(x.pow(2).sum(-1))</p><p>Amplitude is the strength of each frequency bin which is the one visualized in spectrograms.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>D_amp <span style=color:#7a8478>=</span> np<span style=color:#7a8478>.</span>abs(D)
</span></span></code></pre></div><h3 id=logarithmic-transformation>Logarithmic Transformation<a hidden class=anchor aria-hidden=true href=#logarithmic-transformation>#</a></h3><p>Taking log of these amplitude values gives values that are more suitable to how humans perceive audio and relative difference between different frequency strengths. It won&rsquo;t be exact log but close enough (actual formula will look like: 20 * log10(amplitude / reference)). The ref=np.max is basically used to normalize the audio to 1 range and take the log which shifts the values to negative side and max value will be 0, and this is done for conventional reasons.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>S_db <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>amplitude_to_db(D_amp, ref<span style=color:#7a8478>=</span>np<span style=color:#7a8478>.</span>max)
</span></span></code></pre></div><h3 id=display-spectrogram>Display Spectrogram<a hidden class=anchor aria-hidden=true href=#display-spectrogram>#</a></h3><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>librosa<span style=color:#7a8478>.</span>display<span style=color:#7a8478>.</span>specshow(S_db, x_axis<span style=color:#7a8478>=</span><span style=color:#b2c98f>&#39;time&#39;</span>, y_axis<span style=color:#7a8478>=</span><span style=color:#b2c98f>&#39;hz&#39;</span>, hop_length<span style=color:#7a8478>=</span>hop_length)
</span></span><span style=display:flex><span>plt<span style=color:#7a8478>.</span>colorbar(<span style=color:#d699b6>format</span><span style=color:#7a8478>=</span><span style=color:#b2c98f>&#39;</span><span style=color:#b2c98f>%+2.0f</span><span style=color:#b2c98f> dB&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#7a8478>.</span>show()
</span></span></code></pre></div><p><img alt="Plot Description" loading=lazy src=/images/spectrogram.png></p><h3 id=audio-reconstruction-with-griffin-lim-algorithm>Audio Reconstruction with Griffin-Lim Algorithm<a hidden class=anchor aria-hidden=true href=#audio-reconstruction-with-griffin-lim-algorithm>#</a></h3><p>If we want to convert this spectrogram back to the original audio signal, we face one caveat: during amplitude calculation we discarded the phase information. Phase information is crucial for the reconstruction.</p><p>One classical way to estimate that missing phase information is using an iterative algorithm which works like this:</p><ol><li>Assign random phase for the magnitude</li><li>Calculate inverse short time Fourier transform (ISTFT) which will give audio signal</li><li>Now calculate spectrogram from this new audio signal</li><li>Compare the magnitude with original magnitude (kind of loss)</li><li>Now estimate the new phase which reduces this loss</li><li>Repeat this until loss is low</li></ol><p>This algorithm is called the Griffin-Lim algorithm.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>D_amp_reconstructed <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>db_to_amplitude(S_db)
</span></span><span style=display:flex><span>reconstructed_audio <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>griffinlim(D_amp_reconstructed, n_iter<span style=color:#7a8478>=</span><span style=color:#d699b6>32</span>,
</span></span><span style=display:flex><span>                                         hop_length<span style=color:#7a8478>=</span>hop_length,
</span></span><span style=display:flex><span>                                         win_length<span style=color:#7a8478>=</span>n_fft)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#34;reconstructed audio shape: </span><span style=color:#b2c98f>{</span>reconstructed_audio<span style=color:#7a8478>.</span>shape<span style=color:#b2c98f>}</span><span style=color:#b2c98f>&#34;</span>)
</span></span><span style=display:flex><span>ipy<span style=color:#7a8478>.</span>display<span style=color:#7a8478>.</span>Audio(reconstructed_audio, rate<span style=color:#7a8478>=</span>sr)
</span></span></code></pre></div><div class=custom-html-container><figure><figcaption>Reconstructed Audio from Spectrogram</figcaption><audio controls src=/audio/reconstructed_libri1_voice.wav style=width:100%><a href=/audio/reconstructed_libri1_voice.wav>Download the reconstructed audio file</a></audio></figure></div><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("");if(e){const t=e.getElementsByTagName("script");for(let e=0;e<t.length;e++){const n=document.createElement("script");t[e].src?n.src=t[e].src:n.textContent=t[e].textContent,document.body.appendChild(n)}}})</script><p>You can hear that the reconstructed audio is not exactly the same as the original. This is because the phase information was estimated rather than preserved from the original signal. The Griffin-Lim algorithm provides a reasonable approximation, but there will always be some differences compared to the original audio.</p><h3 id=mel-spectrogram-generation>Mel Spectrogram Generation<a hidden class=anchor aria-hidden=true href=#mel-spectrogram-generation>#</a></h3><p>For mel spectrogram we have to take STFT output and convert to amplitude and take the power (which basically squares all the values). The reason why we take square is we want energy which is square of the amplitude. The reason why we need energy instead of amplitude is because human perception is better with power than amplitude.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e67e80>from</span> librosa.filters <span style=color:#e67e80>import</span> mel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>power_sepc <span style=color:#7a8478>=</span> np<span style=color:#7a8478>.</span>abs(D)<span style=color:#7a8478>**</span><span style=color:#d699b6>2</span>
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#34;power spec shape: </span><span style=color:#b2c98f>{</span>power_sepc<span style=color:#7a8478>.</span>shape<span style=color:#b2c98f>}</span><span style=color:#b2c98f>&#34;</span>)
</span></span></code></pre></div><p>Mel filter bank is basically dimensionality reduction of the power spectrogram. These filters are created as follows:</p><ol><li>We take 80 evenly spaced points from fmin=(0) to fmax= sr/2</li><li>Note these evenly spaced points are in log scale (exact formula: 2595 * log10(1 + hz/700))</li><li>Now take these log scale 80 points to linear scale (exact conversion formula: 700 * (10^(mel/2595) - 1))</li><li>This will be the center points for the filters in our frequency bins</li><li>These filter values are peak at the center point and low at the edges like triangle</li><li>Then we apply this filter across frequency bins dimension to get the final bin value</li></ol><p>Basically lower frequency weight reduction will be steep because human perception in lower frequency range is good but higher frequency weight reduction will decrease slowly because in higher frequency range human perception of distinction becomes hard.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n_mels <span style=color:#7a8478>=</span> <span style=color:#d699b6>80</span>
</span></span><span style=display:flex><span>mel_filterbank <span style=color:#7a8478>=</span> mel(sr<span style=color:#7a8478>=</span>sr, n_fft<span style=color:#7a8478>=</span>n_fft, n_mels<span style=color:#7a8478>=</span>n_mels, fmin<span style=color:#7a8478>=</span><span style=color:#d699b6>0</span>, fmax<span style=color:#7a8478>=</span>sr<span style=color:#7a8478>/</span><span style=color:#d699b6>2</span>)
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#34;mel filter bank shape: </span><span style=color:#b2c98f>{</span>mel_filterbank<span style=color:#7a8478>.</span>shape<span style=color:#b2c98f>}</span><span style=color:#b2c98f>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mel_spectogram <span style=color:#7a8478>=</span> np<span style=color:#7a8478>.</span>dot(mel_filterbank, power_sepc)
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#34;mel spec shape: </span><span style=color:#b2c98f>{</span>mel_spectogram<span style=color:#7a8478>.</span>shape<span style=color:#b2c98f>}</span><span style=color:#b2c98f>&#34;</span>)
</span></span></code></pre></div><p>Let&rsquo;s convert back to dB scale. This is to match human loudness perception.</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#859289;font-style:italic># this to match human loudness perception matching</span>
</span></span><span style=display:flex><span>mel_S_db <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>power_to_db(mel_spectogram, ref<span style=color:#7a8478>=</span>np<span style=color:#7a8478>.</span>max)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>librosa<span style=color:#7a8478>.</span>display<span style=color:#7a8478>.</span>specshow(mel_S_db, x_axis<span style=color:#7a8478>=</span><span style=color:#b2c98f>&#39;time&#39;</span>, y_axis<span style=color:#7a8478>=</span><span style=color:#b2c98f>&#39;mel&#39;</span>, sr<span style=color:#7a8478>=</span>sr, hop_length<span style=color:#7a8478>=</span>hop_length)
</span></span><span style=display:flex><span>plt<span style=color:#7a8478>.</span>colorbar(<span style=color:#d699b6>format</span><span style=color:#7a8478>=</span><span style=color:#b2c98f>&#39;</span><span style=color:#b2c98f>%+2.0f</span><span style=color:#b2c98f> dB&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#7a8478>.</span>tight_layout()
</span></span><span style=display:flex><span>plt<span style=color:#7a8478>.</span>show()
</span></span></code></pre></div><figure><img loading=lazy src=/images/melspectrogram.png alt="Mel Spectrogram"><figcaption><p>Figure: Mel Spectrogram with 80 mel bands. Notice how the frequency axis is now non-linear with more resolution in lower frequencies.</p></figcaption></figure><p>You can see how the Mel spectrogram differs from the regular STFT spectrogram we generated earlier. The Mel spectrogram has more resolution in the lower frequency ranges, which better matches human perception of sound. This is why Mel spectrograms are commonly used in speech recognition and music analysis applications.</p><h3 id=mel-spectrogram-inversion>Mel Spectrogram Inversion<a hidden class=anchor aria-hidden=true href=#mel-spectrogram-inversion>#</a></h3><p>Now let&rsquo;s try to convert our mel spectrogram back to audio:</p><div class=highlight><pre tabindex=0 style=color:#d6cbb4;background-color:#252b2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#859289;font-style:italic># power to db</span>
</span></span><span style=display:flex><span>mel_power <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>db_to_power(mel_S_db)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#859289;font-style:italic># mel filterbank which is used for the conversion.</span>
</span></span><span style=display:flex><span>mel_fb <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>filters<span style=color:#7a8478>.</span>mel(sr<span style=color:#7a8478>=</span>sr, n_fft<span style=color:#7a8478>=</span>n_fft, n_mels<span style=color:#7a8478>=</span>n_mels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#859289;font-style:italic># take inverse of the mel. </span>
</span></span><span style=display:flex><span><span style=color:#859289;font-style:italic># this will be less accurate as after mel conversion we lost some of the original information.</span>
</span></span><span style=display:flex><span>mel_inverse <span style=color:#7a8478>=</span> np<span style=color:#7a8478>.</span>linalg<span style=color:#7a8478>.</span>pinv(mel_fb)
</span></span><span style=display:flex><span>linear_spec_approx <span style=color:#7a8478>=</span> np<span style=color:#7a8478>.</span>dot(mel_inverse, mel_power)
</span></span><span style=display:flex><span><span style=color:#d699b6>print</span>(<span style=color:#b2c98f>f</span><span style=color:#b2c98f>&#39;linear spec approximate shape: </span><span style=color:#b2c98f>{</span>linear_spec_approx<span style=color:#7a8478>.</span>shape<span style=color:#b2c98f>}</span><span style=color:#b2c98f>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#859289;font-style:italic># power can&#39;t have negative values. so clip it.</span>
</span></span><span style=display:flex><span>linear_spec_approx <span style=color:#7a8478>=</span> np<span style=color:#7a8478>.</span>maximum(<span style=color:#d699b6>0</span>, linear_spec_approx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#859289;font-style:italic># this reconstruction may not be as accurate like the original direct spec to audio</span>
</span></span><span style=display:flex><span><span style=color:#859289;font-style:italic># conversion because we lose information when we do this mel conversion.</span>
</span></span><span style=display:flex><span>mel_reconstructed_audio <span style=color:#7a8478>=</span> librosa<span style=color:#7a8478>.</span>griffinlim(linear_spec_approx, n_iter<span style=color:#7a8478>=</span><span style=color:#d699b6>32</span>, hop_length<span style=color:#7a8478>=</span>hop_length,
</span></span><span style=display:flex><span>                                             win_length<span style=color:#7a8478>=</span>n_fft)
</span></span><span style=display:flex><span>ipy<span style=color:#7a8478>.</span>display<span style=color:#7a8478>.</span>Audio(mel_reconstructed_audio, rate<span style=color:#7a8478>=</span>sr)
</span></span></code></pre></div><div class=custom-html-container><figure><figcaption>Audio Reconstructed from Mel Spectrogram</figcaption><audio controls src=/audio/mel_reconstructed_libri1_voice.wav style=width:100%><a href=/audio/mel_reconstructed_libri1_voice.wav>Download the Mel reconstructed audio file</a></audio></figure></div><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("");if(e){const t=e.getElementsByTagName("script");for(let e=0;e<t.length;e++){const n=document.createElement("script");t[e].src?n.src=t[e].src:n.textContent=t[e].textContent,document.body.appendChild(n)}}})</script><p>Notice how the audio quality from the Mel spectrogram reconstruction is generally lower than the reconstruction from the original STFT. This is because the Mel transformation is a dimensionality reduction process - we compress the frequency information from the full STFT bins (which was 513 frequency bins) down to just 80 Mel bands.</p><p>The process involves:</p><ol><li>Converting our dB scale Mel spectrogram back to power</li><li>Getting the original Mel filterbank matrix</li><li>Computing the pseudo-inverse of the Mel filterbank</li><li>Using the pseudo-inverse to estimate the original linear spectrogram</li><li>Ensuring all values are non-negative (power can&rsquo;t be negative)</li><li>Using Griffin-Lim algorithm to estimate the phase and reconstruct the audio</li></ol><p>The Mel transformation deliberately discards information that isn&rsquo;t perceptually significant to humans, making it excellent for tasks like speech recognition but less suitable for high-fidelity audio reproduction.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><p>Wikipedia. &ldquo;Nyquist–Shannon sampling theorem.&rdquo; [Online]. Available: <a href=https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem>https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem</a></p></li><li><p>Hugging Face. &ldquo;Audio Course - Introduction.&rdquo; [Online]. Available: <a href=https://huggingface.co/learn/audio-course/chapter0/introduction>https://huggingface.co/learn/audio-course/chapter0/introduction</a></p></li><li><p>3Blue1Brown. &ldquo;But what is the Fourier Transform? A visual introduction.&rdquo; YouTube. [Online]. Available: <a href=https://youtu.be/spUNpyF58BY>https://youtu.be/spUNpyF58BY</a></p></li><li><p>Librosa Documentation. [Online]. Available: <a href=https://librosa.org/doc/latest/index.html>https://librosa.org/doc/latest/index.html</a></p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://arun477.github.io/tags/audio/>Audio</a></li><li><a href=https://arun477.github.io/tags/signal-processing/>Signal-Processing</a></li></ul><nav class=paginav><a class=next href=https://arun477.github.io/posts/embedding_eval_for_indic/><span class=title>Next »</span><br><span>Bharat-NanoBEIR: Indian Language Information Retrieval Dataset</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Mel Spectrogram on x" href="https://x.com/intent/tweet/?text=Mel%20Spectrogram&amp;url=https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f&amp;hashtags=audio%2csignal-processing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Mel Spectrogram on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f&amp;title=Mel%20Spectrogram&amp;summary=Mel%20Spectrogram&amp;source=https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Mel Spectrogram on reddit" href="https://reddit.com/submit?url=https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f&title=Mel%20Spectrogram"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Mel Spectrogram on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Mel Spectrogram on whatsapp" href="https://api.whatsapp.com/send?text=Mel%20Spectrogram%20-%20https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Mel Spectrogram on telegram" href="https://telegram.me/share/url?text=Mel%20Spectrogram&amp;url=https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Mel Spectrogram on ycombinator" href="https://news.ycombinator.com/submitlink?t=Mel%20Spectrogram&u=https%3a%2f%2farun477.github.io%2fposts%2fmel_spectrogram%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://arun477.github.io/>Arun</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>