[{"content":"A Chrome extension that extracts content from Jupyter notebooks for use with large language models.\nFeatures Extract both code and markdown cells Filter out empty cells Apply prompt templates Select specific cells for extraction Copy to clipboard or download as text file Dark mode support Installation Note: This extension is not yet available on the Chrome Web Store.\nManual Installation (Currently the only option) Download the repository as a ZIP file and extract it Open Chrome and navigate to chrome://extensions/ Enable \u0026ldquo;Developer mode\u0026rdquo; (toggle in the top-right corner) Click \u0026ldquo;Load unpacked\u0026rdquo; and select the extracted folder Usage Navigate to any Jupyter notebook Click the extension icon in your toolbar Choose your extraction options: Select which cell types to include Choose a prompt template (optional) Select specific cells (optional) Click \u0026ldquo;Copy to Clipboard\u0026rdquo; or \u0026ldquo;Download as File\u0026rdquo; Development The extension consists of four main components:\npopup.html/js: User interface content.js: Extracts notebook content from the page background.js: Handles file downloads manifest.json: Extension configuration Privacy Note All content extraction happens locally in your browser. No notebook content is ever sent to any server. The extension only requires permissions to access the current tab and handle downloads.\nSource Code The complete source code is available on GitHub. Contributions, issues, and feature requests are welcome.\n","permalink":"https://arun477.github.io/utilities/jupyter_extractor/","summary":"Extract and format Jupyter notebook content for LLMs with support for cell filtering, templates, and selective extraction.","title":"Jupyter to LLM Extractor"},{"content":"llmdirtree: Directory Visualization and Context Generator for LLM Workflows llmdirtree is a specialized developer tool with two powerful capabilities:\nDirectory Tree Visualization - Creates clean, structured representations of your codebase LLM Context Generation - Produces AI-powered summaries of your code while respecting security boundaries Both features are designed to enhance interactions with Large Language Models when discussing or getting help with your code.\nInstallation Install directly from PyPI:\npip install llmdirtree Key Features Directory Tree Visualization Clean, standardized output with Unicode box-drawing characters Intelligent filtering to exclude irrelevant directories (node_modules, .git, etc.) Efficient memory usage by providing structural context without full codebase uploads Full .gitignore integration ensuring sensitive files are excluded from visualization LLM Context Generation AI-powered code analysis via OpenAI\u0026rsquo;s API Security-focused with automatic .gitignore pattern recognition Rich contextual summaries of your codebase organized by directory Intelligent large file handling with automatic chunking and summarization Customizable model selection to choose your preferred OpenAI model Dependency-free implementation using system curl instead of heavy libraries Usage Basic Directory Tree # Generate a simple directory tree llmdirtree --root /path/to/project --output project_structure.txt With LLM Context Generation # Generate both directory tree AND code context llmdirtree --root /path/to/project --llm-context --openai-key YOUR_API_KEY This creates two files:\ndirectory_tree.txt - Visual structure llmcontext.txt - AI-generated project overview and file summaries Advanced Options # Exclude specific directories llmdirtree --exclude node_modules .git venv dist # Customize output locations llmdirtree --output custom_tree.txt --context-output custom_context.txt # Control file selection for context generation llmdirtree --max-files 150 --llm-context # Override gitignore protection (not recommended) llmdirtree --ignore-gitignore --llm-context # Specify OpenAI model (skips prompting) llmdirtree --llm-context --model gpt-4 Example Outputs Directory Tree Directory Tree for: /project Excluding: .git, node_modules, __pycache__, venv -------------------------------------------------- project/ ├── src/ │ ├── main.py │ └── utils/ │ └── helpers.py ├── tests/ │ └── test_main.py └── README.md LLM Context File # project-name \u0026gt; A React web application for tracking personal fitness goals with a Node.js backend and MongoDB database. ## src/components/ - **Dashboard.jsx**: Main dashboard component that displays user fitness stats, recent activities, and goal progress. - **WorkoutForm.jsx**: Form for creating and editing workout entries with validation and submission handling. ## src/utils/ - **api.js**: Contains functions for making API calls to the backend, handling authentication and data fetching. - **formatters.js**: Utility functions for formatting dates, weights, and other fitness metrics consistently. Example Workflow Directory Tree Only Generate your project structure:\nllmdirtree --root /path/to/project --output structure.txt Include it in your LLM prompt:\nHere\u0026#39;s my project structure: [paste content from structure.txt] I need help understanding how the components interact... With Full Context Generate both structure and context:\nllmdirtree --root /path/to/project --llm-context Include in your LLM prompt:\nHere\u0026#39;s information about my project: [paste content from llmcontext.txt] Now I need help with implementing a new feature... The LLM now has both structural and semantic understanding of your code.\nLarge File Handling One of the challenges when analyzing code for LLM context is dealing with large files that exceed token limits. llmdirtree now intelligently processes large files by:\nAutomatic file chunking - Breaking large files into semantically meaningful chunks Per-chunk summarization - Analyzing each chunk independently Cohesive synthesis - Creating a unified summary that captures the essence of the entire file This approach ensures even large files like complex modules or extensive configuration are appropriately represented in your context.\nOpenAI Model Selection llmdirtree offers flexible model selection to balance performance and cost:\nInteractive selection - By default, you\u0026rsquo;ll be prompted once per run to select a model Command-line specification - Skip the prompt with --model MODEL_NAME Sensible defaults - Uses gpt-3.5-turbo-16k for batch processing and gpt-3.5-turbo for project overview This flexibility allows you to choose the right model for your specific needs, whether optimizing for speed, token capacity, or analysis quality.\n.gitignore Support The tool now fully respects .gitignore patterns for both the directory tree visualization and context generation:\nComplete pattern recognition - Supports all standard .gitignore syntax including wildcards Directory-level protection - Skips entire directories that match ignore patterns Consistent application - Same ignore logic applied to both tree visualization and context generation This ensures sensitive files or directories are completely excluded from any visualization or analysis.\nSecurity Features The tool takes security seriously:\nRespects .gitignore patterns to avoid exposing sensitive information Excludes credentials and API keys automatically Zero dependencies approach for core functionality Configurable privacy with options to control what\u0026rsquo;s analyzed For more implementation details or to contribute, check the GitHub repository.\n","permalink":"https://arun477.github.io/utilities/directory-tree-generator/","summary":"Enhance your AI coding assistant experience with rich project context that respects privacy and security boundaries.","title":"llmdirtree: Directory Visualization and Context Generator for LLM Workflows"},{"content":"Introduction Mel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models\u0026rsquo; input raw audio is converted into mel spectrogram before passing to the model.\nIn general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form.\nTo fully understand how this conversion process occurs, what exactly is going on under the hood, and why we need this conversion, we need to understand a few other related concepts and techniques surrounding audio in general.\nHow Audio is Represented Digitally In a physical sense, audio signals are continuous signals, but we usually represent this audio in discrete form when we represent it digitally. It is basically a list of numbers, usually 16-bit integers (if it\u0026rsquo;s a .wav file).\nEach number in this list basically records the amplitude (strength of the physical sound waves) value captured by the microphone at various time points.\nSampling Rate Now here, one parameter comes into the picture, which is how many samples the microphone should record in a given time period. For example, if our microphone records a lot of samples per second, then our audio quality would be much better, but there is a limit. This is usually 22k samples per second or 44k samples per second. These rates are enough to capture most of the audio frequency and quality. This rate is called the sampling rate.\nNyquist Theorem (Mathematical Guarantee) Now another question comes up: why do these choices usually work, like 22k samples per second or 44k samples per second? There is one famous mathematical theorem called the Nyquist theorem, which provides strong theoretical guarantees. It says that if you want to capture higher frequencies, then your sampling rate should be 2 times the highest frequency you are trying to capture.\n$$f_{max} = \\frac{f_s}{2}$$\nWhere: $$\\begin{aligned} f_{max} \u0026amp;= \\text{highest frequency (Nyquist frequency)} \\ f_s \u0026amp;= \\text{sampling rate (Hz)} \\end{aligned}$$\nUsually, the human hearing range is 20 Hz to 20 kHz, so a sampling rate of 40 kHz will capture up to 20 kHz. This is enough for most cases. Sometimes music requires a slightly higher frequency range, so accordingly, a higher sampling rate is required.\nNotational Meaning Difference There is a slight notational difference here. Hz means different things when we talk about frequency and sampling rate. When we say 40 kHz, we are talking about 40k samples per second, and when we say 40 kHz in frequency, we are talking about cycles per second.\nTime Domain Signal Now this array of signal captures all required information of the audio signal. We can transmit it and play it using a speaker. This is called time domain signal because it is basically a signal across the time.\nFrequency Domain Signal One of the interesting and key ideas is that we can pick a particular instant in time and take the samples from that instant to check what are all the frequencies available.\nThis is done through another cool mathematical technique called Fourier transform which takes the samples of the audio at a particular instant in time and gives all the available frequency information at that instant. To be exact, it will give all the frequencies with their magnitude and phase information (usually complex numbers).\nOf course, the maximum frequency it can extract is bound by Nyquist theorem, basically sampling rate / 2 is the maximum frequency we can extract.\nSpectrogram Now if we take this frequency information across different time instants, we will get frequency information at each instant of time across the time axis. This is called spectrogram.\nWe usually do two additional transformations on this spectrogram.\nTaking only amplitude value. Fourier transform returns frequency value as a complex number which contains both amplitude and phase information of each frequency. Usually, we discard phase information and only keep amplitude information. This is done by taking the Euclidean norm of the real and imaginary parts.\nFinal transform is converting this amplitude value to decibel value, basically taking each amplitude value and taking the log of it. The reason seems to be that this closely matches how humans perceive the audio amplitude. To be exact, the following is the transformation formula.\n$$\\text{dB} = 20 \\log_{10}\\left(\\frac{|A|}{A_{\\text{ref}}}\\right)$$\nWhere: $$\\begin{aligned} \\text{dB} \u0026amp;= \\text{amplitude in decibels} \\ |A| \u0026amp;= \\text{absolute value of amplitude} \\ A_{\\text{ref}} \u0026amp;= \\text{reference amplitude value} \\end{aligned}$$\n$$\\text{dB} = 10 \\log_{10}\\left(\\frac{|X(f)|^2}{X_{\\text{ref}}^2}\\right)$$\nOr equivalently: $$\\text{dB} = 20 \\log_{10}\\left(\\frac{|X(f)|}{X_{\\text{ref}}}\\right)$$\nWhere: $$\\begin{aligned} X(f) \u0026amp;= \\text{Fourier transform at frequency } f \\ |X(f)| \u0026amp;= \\text{magnitude of the transform} \\ X_{\\text{ref}} \u0026amp;= \\text{reference value} \\end{aligned}$$\nMel Spectrogram The spectrogram we have seen so far gives information about each instant, what are all the frequencies, and how it changes across the time.\nThere is another interesting fact: humans can\u0026rsquo;t very well differentiate between higher frequencies the way they do in lower frequency ranges. Basically, human frequency perception kind of follows a logarithmic function which implies if we take one instant of spectrogram frequency information, they usually have linear spacing, meaning 20 Hz, 40 Hz, 60 Hz, etc. (in this example 20 Hz apart). But as we move to higher frequency ranges, the human ear can\u0026rsquo;t distinguish this. Meaning the human ear can distinguish the difference between 20 Hz and 40 Hz but it can\u0026rsquo;t between 12 kHz and 12.20 kHz, even though the same space between both frequencies. So there is a lot of unnecessary frequency information (in terms of human perception point of view).\nWhich implies we can group a lot of the frequency based on this idea. This is done by first picking a set of frequency bands, usually 80 bands, which basically focus on different frequency ranges from 0 to max frequency. Each frequency band will have a set of weights which tries to capture human perception. Lower frequency weights will be mostly zero weights for far away frequencies and less weights for nearby frequencies. In higher frequency bands, weights will be dispersed because there we can combine a lot of frequencies.\nThis mel spectrogram process might not be obvious but when we go through the full code it will get clarified. But for now, it\u0026rsquo;s kind of a dimensionality reduction technique on spectrogram which takes into account how humans perceive sounds.\nCode Let\u0026rsquo;s do the full conversion process in code to see what exactly is going on. For most of the audio processing, let\u0026rsquo;s use a library called librosa which contains a lot of useful utility functions to work with the audio data and along with some sample audio data to work with.\nFirst, let\u0026rsquo;s import required libraries and let\u0026rsquo;s use libri1 sample audio from librosa library which is basically human voice.\nimport librosa import numpy as np import matplotlib.pyplot as plt import IPython as ipy audio, sr = librosa.load(librosa.ex(\u0026#39;libri1\u0026#39;)) In the above code, sr is a sampling rate and audio is our actual audio data which is a 1-dimensional array of numbers.\nFollowing is the original audio.\nYour browser does not support the audio element. Setup FFT Parameters n_fft = 1024 hop_length = 512 Frequency Resolution We know maximum frequency possible by Nyquist theorem is sampling_rate/2, so FFT can produce up to sampling_rate/2 frequency. Another fact is FFT algorithm produces symmetric values so only n_fft/2 bins are unique frequency bins. FFT algorithm works by evenly spaced frequency bins, so the spacing becomes (sampling_rate/2) / (n_fft/2) = sampling_rate/n_fft.\nfrq_resolution = sr/n_fft print(f\u0026#39;frq resolution: {frq_resolution:.0f} Hz\u0026#39;) Time Resolution 1/sampling_rate will tell us spacing between individual sample points. n_fft * (1/sampling_rate) will tell us the time bracket the FFT instance occupies.\ntime_resolution = n_fft/sr print(f\u0026#39;time resolution: {time_resolution*1000:.0f} ms\u0026#39;) There is a trade-off here. If we increase the time resolution, then we have to sacrifice the frequency resolution. So balance is required.\nFrequency Bins Calculation freq_bins = (n_fft//2) + 1 print(f\u0026#34;frequency bins: {freq_bins}\u0026#34;) Time Frame Calculation How many time frames for the given audio signals after STFT. This is like convolution kernel operation.\ntotal_time_frame_points = int((1+(len(audio)-n_fft)/hop_length)) print(f\u0026#39;estimated total time frame points: {total_time_frame_points:.0f}\u0026#39;) Short-Time Fourier Transform (STFT) D = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length) print(f\u0026#34;stft output: {D.shape} = (frq_bins, time_frame_points)\u0026#34;) Amplitude Extraction STFT contains both amplitude and phase information. Usually we take only amplitude information and discard the phase information. There is some subtlety here - np.abs for real part works just by taking normal abs, but if it\u0026rsquo;s a complex number, in this case it will calculate abs by taking Euclidean norm of the real and complex parts. Equivalent in PyTorch would be the following: x = torch.tensor([D[1][0].real, D[1][0].imag]) torch.sqrt(x.pow(2).sum(-1))\nAmplitude is the strength of each frequency bin which is the one visualized in spectrograms.\nD_amp = np.abs(D) Logarithmic Transformation Taking log of these amplitude values gives values that are more suitable to how humans perceive audio and relative difference between different frequency strengths. It won\u0026rsquo;t be exact log but close enough (actual formula will look like: 20 * log10(amplitude / reference)). The ref=np.max is basically used to normalize the audio to 1 range and take the log which shifts the values to negative side and max value will be 0, and this is done for conventional reasons.\nS_db = librosa.amplitude_to_db(D_amp, ref=np.max) Display Spectrogram librosa.display.specshow(S_db, x_axis=\u0026#39;time\u0026#39;, y_axis=\u0026#39;hz\u0026#39;, hop_length=hop_length) plt.colorbar(format=\u0026#39;%+2.0f dB\u0026#39;) plt.show() Audio Reconstruction with Griffin-Lim Algorithm If we want to convert this spectrogram back to the original audio signal, we face one caveat: during amplitude calculation we discarded the phase information. Phase information is crucial for the reconstruction.\nOne classical way to estimate that missing phase information is using an iterative algorithm which works like this:\nAssign random phase for the magnitude Calculate inverse short time Fourier transform (ISTFT) which will give audio signal Now calculate spectrogram from this new audio signal Compare the magnitude with original magnitude (kind of loss) Now estimate the new phase which reduces this loss Repeat this until loss is low This algorithm is called the Griffin-Lim algorithm.\nD_amp_reconstructed = librosa.db_to_amplitude(S_db) reconstructed_audio = librosa.griffinlim(D_amp_reconstructed, n_iter=32, hop_length=hop_length, win_length=n_fft) print(f\u0026#34;reconstructed audio shape: {reconstructed_audio.shape}\u0026#34;) ipy.display.Audio(reconstructed_audio, rate=sr) Reconstructed Audio from Spectrogram Download the reconstructed audio file You can hear that the reconstructed audio is not exactly the same as the original. This is because the phase information was estimated rather than preserved from the original signal. The Griffin-Lim algorithm provides a reasonable approximation, but there will always be some differences compared to the original audio.\nMel Spectrogram Generation For mel spectrogram we have to take STFT output and convert to amplitude and take the power (which basically squares all the values). The reason why we take square is we want energy which is square of the amplitude. The reason why we need energy instead of amplitude is because human perception is better with power than amplitude.\nfrom librosa.filters import mel power_sepc = np.abs(D)**2 print(f\u0026#34;power spec shape: {power_sepc.shape}\u0026#34;) Mel filter bank is basically dimensionality reduction of the power spectrogram. These filters are created as follows:\nWe take 80 evenly spaced points from fmin=(0) to fmax= sr/2 Note these evenly spaced points are in log scale (exact formula: 2595 * log10(1 + hz/700)) Now take these log scale 80 points to linear scale (exact conversion formula: 700 * (10^(mel/2595) - 1)) This will be the center points for the filters in our frequency bins These filter values are peak at the center point and low at the edges like triangle Then we apply this filter across frequency bins dimension to get the final bin value Basically lower frequency weight reduction will be steep because human perception in lower frequency range is good but higher frequency weight reduction will decrease slowly because in higher frequency range human perception of distinction becomes hard.\nn_mels = 80 mel_filterbank = mel(sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=0, fmax=sr/2) print(f\u0026#34;mel filter bank shape: {mel_filterbank.shape}\u0026#34;) mel_spectogram = np.dot(mel_filterbank, power_sepc) print(f\u0026#34;mel spec shape: {mel_spectogram.shape}\u0026#34;) Let\u0026rsquo;s convert back to dB scale. This is to match human loudness perception.\n# this to match human loudness perception matching mel_S_db = librosa.power_to_db(mel_spectogram, ref=np.max) librosa.display.specshow(mel_S_db, x_axis=\u0026#39;time\u0026#39;, y_axis=\u0026#39;mel\u0026#39;, sr=sr, hop_length=hop_length) plt.colorbar(format=\u0026#39;%+2.0f dB\u0026#39;) plt.tight_layout() plt.show() Figure: Mel Spectrogram with 80 mel bands. Notice how the frequency axis is now non-linear with more resolution in lower frequencies.\nYou can see how the Mel spectrogram differs from the regular STFT spectrogram we generated earlier. The Mel spectrogram has more resolution in the lower frequency ranges, which better matches human perception of sound. This is why Mel spectrograms are commonly used in speech recognition and music analysis applications.\nMel Spectrogram Inversion Now let\u0026rsquo;s try to convert our mel spectrogram back to audio:\n# power to db mel_power = librosa.db_to_power(mel_S_db) # mel filterbank which is used for the conversion. mel_fb = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels) # take inverse of the mel. # this will be less accurate as after mel conversion we lost some of the original information. mel_inverse = np.linalg.pinv(mel_fb) linear_spec_approx = np.dot(mel_inverse, mel_power) print(f\u0026#39;linear spec approximate shape: {linear_spec_approx.shape}\u0026#39;) # power can\u0026#39;t have negative values. so clip it. linear_spec_approx = np.maximum(0, linear_spec_approx) # this reconstruction may not be as accurate like the original direct spec to audio # conversion because we lose information when we do this mel conversion. mel_reconstructed_audio = librosa.griffinlim(linear_spec_approx, n_iter=32, hop_length=hop_length, win_length=n_fft) ipy.display.Audio(mel_reconstructed_audio, rate=sr) Audio Reconstructed from Mel Spectrogram Download the Mel reconstructed audio file Notice how the audio quality from the Mel spectrogram reconstruction is generally lower than the reconstruction from the original STFT. This is because the Mel transformation is a dimensionality reduction process - we compress the frequency information from the full STFT bins (which was 513 frequency bins) down to just 80 Mel bands.\nThe process involves:\nConverting our dB scale Mel spectrogram back to power Getting the original Mel filterbank matrix Computing the pseudo-inverse of the Mel filterbank Using the pseudo-inverse to estimate the original linear spectrogram Ensuring all values are non-negative (power can\u0026rsquo;t be negative) Using Griffin-Lim algorithm to estimate the phase and reconstruct the audio The Mel transformation deliberately discards information that isn\u0026rsquo;t perceptually significant to humans, making it excellent for tasks like speech recognition but less suitable for high-fidelity audio reproduction.\nReferences Wikipedia. \u0026ldquo;Nyquist–Shannon sampling theorem.\u0026rdquo; [Online]. Available: https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\nHugging Face. \u0026ldquo;Audio Course - Introduction.\u0026rdquo; [Online]. Available: https://huggingface.co/learn/audio-course/chapter0/introduction\n3Blue1Brown. \u0026ldquo;But what is the Fourier Transform? A visual introduction.\u0026rdquo; YouTube. [Online]. Available: https://youtu.be/spUNpyF58BY\nLibrosa Documentation. [Online]. Available: https://librosa.org/doc/latest/index.html\n","permalink":"https://arun477.github.io/posts/mel_spectrogram/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eMel spectrogram is an audio analyzing technique which is predominantly applied to raw audio form as a preprocessing step before passing to any model for predictions. For example, speech-to-text models\u0026rsquo; input raw audio is converted into mel spectrogram before passing to the model.\u003c/p\u003e\n\u003cp\u003eIn general, mel spectrogram is a kind of visualization technique which takes into account how the human ear perceives audio frequencies during this low dimensional conversion process. Compared to the raw audio waveform and the more natural way humans perceive audio, these are all the predominant reasons we prefer audio in mel spectrogram form.\u003c/p\u003e","title":"Mel Spectrogram"},{"content":"The Bharat-NanoBEIR dataset is part of a collection that provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets with 50 queries and up to 10K documents each.\nDataset Overview Queries: 50 per dataset Documents: Up to 10K per dataset This dataset aims to facilitate research and development in the field of information retrieval, particularly for Indian languages.\nLearn More For further details and to explore the dataset, please visit the following link:\nHugging Face Collection: Bharat-NanoBEIR\n","permalink":"https://arun477.github.io/posts/embedding_eval_for_indic/","summary":"\u003cp\u003eThe \u003cstrong\u003eBharat-NanoBEIR\u003c/strong\u003e dataset is part of a collection that provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets with 50 queries and up to 10K documents each.\u003c/p\u003e\n\u003ch3 id=\"dataset-overview\"\u003eDataset Overview\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eQueries:\u003c/strong\u003e 50 per dataset\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDocuments:\u003c/strong\u003e Up to 10K per dataset\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis dataset aims to facilitate research and development in the field of information retrieval, particularly for Indian languages.\u003c/p\u003e\n\u003ch3 id=\"learn-more\"\u003eLearn More\u003c/h3\u003e\n\u003cp\u003eFor further details and to explore the dataset, please visit the following link:\u003c/p\u003e","title":"Bharat-NanoBEIR: Indian Language Information Retrieval Dataset"}]